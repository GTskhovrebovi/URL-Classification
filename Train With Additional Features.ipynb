{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_legitimate = pd.read_csv(\"clean_legitimate.txt\",names=['url'], header = None, sep = 'asdas2131sad12sa1a11a2sa1',engine='python',encoding = 'utf8').dropna()\n",
    "df_malicious = pd.read_csv(\"clean_malicious.txt\",names=['url'], header = None, sep = 'asdas2131sad12sa1a11a2sa1',engine='python',encoding = 'utf8').dropna()\n",
    "X_data = df_legitimate['url'].tolist() + df_malicious['url'].tolist()\n",
    "y_data = np.zeros(len(df_legitimate)).tolist() + np.ones(len(df_malicious)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44698, 1)\n",
      "(1265994, 1)\n"
     ]
    }
   ],
   "source": [
    "print(df_malicious.shape)\n",
    "print(df_legitimate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=0, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# marto sityvebis da ricxvebis datoveba\n",
    "X_train_words = [re.sub(\"[^A-Za-z0-9]\", \" \", i) for i in X_train]\n",
    "X_train_words = [re.sub(' +',' ', i).strip().lower() for i in X_train_words]  \n",
    "X_test_words = [re.sub(\"[^A-Za-z0-9]\", \" \", i) for i in X_test]\n",
    "X_test_words = [re.sub(' +',' ', i).strip().lower() for i in X_test_words] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "x_traincv =cv.fit_transform(X_train_words)\n",
    "x_testcv=cv.transform(X_test_words)\n",
    "\n",
    "x_train_tf_idf =tf_idf.fit_transform(X_train_words)\n",
    "x_test_tf_idf=tf_idf.transform(X_test_words)\n",
    "\n",
    "y_train= [int(i) for i in y_train]\n",
    "y_test= [int(i) for i in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abrunebs featurebis raodenoba X data Size zomis np.arrays\n",
    "def get_additional_features(data):\n",
    "    \n",
    "    a1  = np.asarray([[int(len(i)<=4) for i in data]])\n",
    "    a2  = np.asarray([[int(4<len(i)<=12) for i in data]])\n",
    "    a3  = np.asarray([[int(12<len(i)<=44) for i in data]])\n",
    "    a4  = np.asarray([[int(44<len(i)) for i in data]])\n",
    "\n",
    "    a5  = np.asarray([[i.count('/') for i in data]])\n",
    "    a6  = np.asarray([[i.count('//') for i in data]])\n",
    "    a7  = np.asarray([[i.count('<') for i in data]])\n",
    "    a8  = np.asarray([[i.count('>') for i in data]])\n",
    "    a9  = np.asarray([[i.count('.') for i in data]])\n",
    "    a10 = np.asarray([[i.count(',') for i in data]])\n",
    "    a11 = np.asarray([[i.count('..') for i in data]])\n",
    "    a12 = np.asarray([[i.count('(') for i in data]])\n",
    "    a13 = np.asarray([[i.count(')') for i in data]])\n",
    "    a14 = np.asarray([[i.count('[') for i in data]])\n",
    "    a15 = np.asarray([[i.count(']') for i in data]])\n",
    "    a16 = np.asarray([[i.count('\"') for i in data]])\n",
    "    a17 = np.asarray([[i.count(\"'\") for i in data]])\n",
    "    a18 = np.asarray([[i.count('?') for i in data]])\n",
    "    a19 = np.asarray([[i.count(';') for i in data]])\n",
    "    a20 = np.asarray([[i.count('-') for i in data]])\n",
    "    \n",
    "    a21 = np.asarray([[i.count('https') for i in data]])\n",
    "    a22 = np.asarray([[(i.count('<')>0 and i.count('>')) for i in data]])\n",
    "    a23 = np.asarray([[i.count('%') for i in data]])\n",
    "    a24 = np.asarray([[i.count('+') for i in data]])\n",
    "    a25 = np.asarray([[i.count('</') for i in data]])\n",
    "    a26 = np.asarray([[i.count('.exe') for i in data]])\n",
    "    a27 = np.asarray([[i.count('&') for i in data]])\n",
    "    a28 = np.asarray([[i.count('=') for i in data]])\n",
    "\n",
    "    \n",
    "    return np.concatenate([a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, a16, a17, a18, a19, a20, a21, a22, a23, a24,a25,a26,a27,a28]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featurebis gaertianeba\n",
    "X_train_final = sp.sparse.hstack([x_traincv, get_additional_features(X_train)])\n",
    "X_test_final = sp.sparse.hstack([x_testcv, get_additional_features(X_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mnb_model = MultinomialNB()\n",
    "linear_svc_model = LinearSVC()\n",
    "logistic_model = LogisticRegression()\n",
    "random_forest_model = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_model.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_svc_model.fit(X_train_final, y_train) #~2min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_model.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion(model, X,y):\n",
    "    predictions=model.predict(X)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(predictions, y).ravel()\n",
    "    total = tn+fp+fn+tp\n",
    "    actual_no = tn + fp\n",
    "    actual_yes = tp + fn\n",
    "\n",
    "    print('tn:' + str(tn), 'fp:' + str(fp))\n",
    "    print('fn:' + str(fn), 'tp:' + str(tp) +'\\n')\n",
    "    print('Accuracy: ' + str((tp+tn)*100/total))\n",
    "    print('Misclassification: ' + str((fp+fn)*100/total))\n",
    "    print('True Positive Rate: ' + str((tp)*100/actual_yes))\n",
    "    print('False Positive Rat: ' + str((fp)*100/actual_no))\n",
    "    print('Specificity: ' + str((tn)*100/actual_no))\n",
    "    print('Precision: ' + str((tp)*100/(tp+fp)))\n",
    "    print('Prevalence: ' + str(actual_yes*100/total))\n",
    "    print(tp / (tp+fp))\n",
    "    print(tn / (tn+fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn:252576 fp:1589\n",
      "fn:630 tp:7344\n",
      "\n",
      "Accuracy: 99.15350253109992\n",
      "Misclassification: 0.8464974689000874\n",
      "True Positive Rate: 92.09932279909707\n",
      "False Positive Rat: 0.6251844274388685\n",
      "Specificity: 99.37481557256113\n",
      "Precision: 82.21202283667301\n",
      "Prevalence: 3.0418976192020266\n",
      "0.8221202283667302\n",
      "0.9975119073007749\n"
     ]
    }
   ],
   "source": [
    "confusion(mnb_model, X_test_final, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn:253166 fp:171\n",
      "fn:40 tp:8762\n",
      "\n",
      "Accuracy: 99.91950835243897\n",
      "Misclassification: 0.08049164756102678\n",
      "True Positive Rate: 99.54555782776642\n",
      "False Positive Rat: 0.06749902304045599\n",
      "Specificity: 99.93250097695955\n",
      "Precision: 98.08574946826374\n",
      "Prevalence: 3.357760577403591\n",
      "0.9808574946826374\n",
      "0.9998420258603666\n"
     ]
    }
   ],
   "source": [
    "confusion(linear_svc_model, X_test_final, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn:253161 fp:266\n",
      "fn:45 tp:8667\n",
      "\n",
      "Accuracy: 99.88136065217309\n",
      "Misclassification: 0.11863934782691625\n",
      "True Positive Rate: 99.48347107438016\n",
      "False Positive Rat: 0.10496119198033359\n",
      "Specificity: 99.89503880801966\n",
      "Precision: 97.02227695063249\n",
      "Prevalence: 3.3234276471642907\n",
      "0.9702227695063249\n",
      "0.9998222790929124\n"
     ]
    }
   ],
   "source": [
    "confusion(logistic_model, X_test_final, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion(random_forest_model, X_test_final, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
