{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cxovrika\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_legitimate = pd.read_csv(\"clean_legitimate.txt\",names=['url'], header = None, sep = 'asdas2131sad12sa1a11a2sa1',engine='python',encoding = 'utf8').dropna()\n",
    "df_malicious = pd.read_csv(\"clean_malicious.txt\",names=['url'], header = None, sep = 'asdas2131sad12sa1a11a2sa1',engine='python',encoding = 'utf8').dropna()\n",
    "X_data = df_legitimate['url'].tolist() + df_malicious['url'].tolist()\n",
    "y_data = np.zeros(len(df_legitimate)).tolist() + np.ones(len(df_malicious)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44698, 1)\n",
      "(1265994, 1)\n"
     ]
    }
   ],
   "source": [
    "print(df_malicious.shape)\n",
    "print(df_legitimate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=0, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# marto sityvebis da ricxvebis datoveba\n",
    "X_train_words = [re.sub(\"[^A-Za-z0-9]\", \" \", i) for i in X_train]\n",
    "X_train_words = [re.sub(' +',' ', i).strip().lower() for i in X_train_words]  \n",
    "X_test_words = [re.sub(\"[^A-Za-z0-9]\", \" \", i) for i in X_test]\n",
    "X_test_words = [re.sub(' +',' ', i).strip().lower() for i in X_test_words] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "x_traincv =cv.fit_transform(X_train_words)\n",
    "x_testcv=cv.transform(X_test_words)\n",
    "\n",
    "x_train_tf_idf =tf_idf.fit_transform(X_train_words)\n",
    "x_test_tf_idf=tf_idf.transform(X_test_words)\n",
    "\n",
    "y_train= [int(i) for i in y_train]\n",
    "y_test= [int(i) for i in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abrunebs featurebis raodenoba X data Size zomis np.arrays\n",
    "def get_additional_features(data):\n",
    "    \n",
    "    a1  = np.asarray([[int(len(i)<=4) for i in data]])\n",
    "    a2  = np.asarray([[int(4<len(i)<=12) for i in data]])\n",
    "    a3  = np.asarray([[int(12<len(i)<=44) for i in data]])\n",
    "    a4  = np.asarray([[int(44<len(i)) for i in data]])\n",
    "\n",
    "    a5  = np.asarray([[i.count('/') for i in data]])\n",
    "    a6  = np.asarray([[i.count('//') for i in data]])\n",
    "    a7  = np.asarray([[i.count('<') for i in data]])\n",
    "    a8  = np.asarray([[i.count('>') for i in data]])\n",
    "    a9  = np.asarray([[i.count('.') for i in data]])\n",
    "    a10 = np.asarray([[i.count(',') for i in data]])\n",
    "    a11 = np.asarray([[i.count('..') for i in data]])\n",
    "    a12 = np.asarray([[i.count('(') for i in data]])\n",
    "    a13 = np.asarray([[i.count(')') for i in data]])\n",
    "    a14 = np.asarray([[i.count('[') for i in data]])\n",
    "    a15 = np.asarray([[i.count(']') for i in data]])\n",
    "    a16 = np.asarray([[i.count('\"') for i in data]])\n",
    "    a17 = np.asarray([[i.count(\"'\") for i in data]])\n",
    "    a18 = np.asarray([[i.count('?') for i in data]])\n",
    "    a19 = np.asarray([[i.count(';') for i in data]])\n",
    "    a20 = np.asarray([[i.count('-') for i in data]])\n",
    "    \n",
    "    a21 = np.asarray([[i.count('https') for i in data]])\n",
    "    a22 = np.asarray([[(i.count('<')>0 and i.count('>')) for i in data]])\n",
    "    a23 = np.asarray([[i.count('%') for i in data]])\n",
    "    a24 = np.asarray([[i.count('+') for i in data]])\n",
    "    a25 = np.asarray([[i.count('</') for i in data]])\n",
    "    a26 = np.asarray([[i.count('.exe') for i in data]])\n",
    "    a27 = np.asarray([[i.count('&') for i in data]])\n",
    "    a28 = np.asarray([[i.count('=') for i in data]])\n",
    "\n",
    "    \n",
    "    return np.concatenate([a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, a16, a17, a18, a19, a20, a21, a22, a23, a24,a25,a26,a27,a28]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featurebis gaertianeba\n",
    "X_train_final = sp.sparse.hstack([x_traincv, get_additional_features(X_train)])\n",
    "X_test_final = sp.sparse.hstack([x_testcv, get_additional_features(X_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_model = MultinomialNB()\n",
    "linear_svc_model = LinearSVC()\n",
    "logistic_model = LogisticRegression()\n",
    "random_forest_model = RandomForestClassifier(verbose=3)\n",
    "svc_model = SVC(verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_model.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_svc_model.fit(X_train_final, y_train) #~2min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_model.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 10\n"
     ]
    }
   ],
   "source": [
    "random_forest_model.fit(X_train_final, y_train) # ~30min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "svc_model.fit(X_train_final, y_train)#too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion(model, X,y):\n",
    "    predictions=model.predict(X)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(predictions, y).ravel()\n",
    "    total = tn+fp+fn+tp\n",
    "    actual_no = tn + fp\n",
    "    actual_yes = tp + fn\n",
    "\n",
    "    print('tn:' + str(tn), 'fp:' + str(fp))\n",
    "    print('fn:' + str(fn), 'tp:' + str(tp) +'\\n')\n",
    "    print('Accuracy: ' + str((tp+tn)*100/total))\n",
    "    print('Misclassification: ' + str((fp+fn)*100/total))\n",
    "    print('True Positive Rate: ' + str((tp)*100/actual_yes))\n",
    "    print('False Positive Rat: ' + str((fp)*100/actual_no))\n",
    "    print('Specificity: ' + str((tn)*100/actual_no))\n",
    "    print('Precision: ' + str((tp)*100/(tp+fp)))\n",
    "    print('Prevalence: ' + str(actual_yes*100/total))\n",
    "    print(tp / (tp+fp))\n",
    "    print(tn / (tn+fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn:252576 fp:1589\n",
      "fn:630 tp:7344\n",
      "\n",
      "Accuracy: 99.15350253109992\n",
      "Misclassification: 0.8464974689000874\n",
      "True Positive Rate: 92.09932279909707\n",
      "False Positive Rat: 0.6251844274388685\n",
      "Specificity: 99.37481557256113\n",
      "Precision: 82.21202283667301\n",
      "Prevalence: 3.0418976192020266\n",
      "0.8221202283667302\n",
      "0.9975119073007749\n"
     ]
    }
   ],
   "source": [
    "confusion(mnb_model, X_test_final, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn:253167 fp:170\n",
      "fn:39 tp:8763\n",
      "\n",
      "Accuracy: 99.92027130644429\n",
      "Misclassification: 0.079728693555709\n",
      "True Positive Rate: 99.55691888207225\n",
      "False Positive Rat: 0.06710429191156443\n",
      "Specificity: 99.93289570808844\n",
      "Precision: 98.09694391581776\n",
      "Prevalence: 3.357760577403591\n",
      "0.9809694391581776\n",
      "0.9998459752138575\n"
     ]
    }
   ],
   "source": [
    "confusion(linear_svc_model, X_test_final, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn:253161 fp:266\n",
      "fn:45 tp:8667\n",
      "\n",
      "Accuracy: 99.88136065217309\n",
      "Misclassification: 0.11863934782691625\n",
      "True Positive Rate: 99.48347107438016\n",
      "False Positive Rat: 0.10496119198033359\n",
      "Specificity: 99.89503880801966\n",
      "Precision: 97.02227695063249\n",
      "Prevalence: 3.3234276471642907\n",
      "0.9702227695063249\n",
      "0.9998222790929124\n"
     ]
    }
   ],
   "source": [
    "confusion(logistic_model, X_test_final, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion(svc_model, X_test_final, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    4.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   21.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn:253175 fp:294\n",
      "fn:31 tp:8639\n",
      "\n",
      "Accuracy: 99.87601997413586\n",
      "Misclassification: 0.12398002586414078\n",
      "True Positive Rate: 99.64244521337947\n",
      "False Positive Rat: 0.11599051560545866\n",
      "Specificity: 99.88400948439454\n",
      "Precision: 96.70883241912011\n",
      "Prevalence: 3.3074056130526173\n",
      "0.9670883241912012\n",
      "0.9998775700417841\n"
     ]
    }
   ],
   "source": [
    "confusion(random_forest_model, X_test_final, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_indexes = np.where(y_test != mnb_model.predict(X_test_final))\n",
    "misclassified = [('Malicious' if int(y_test[i]) == 1 else 'Legitimate') + ': ' + X_test[i] for i in misclassified_indexes[0].tolist()]\n",
    "misclassified.sort()\n",
    "misclassified\n",
    "\n",
    "file = open('Missclassified\\Multinomial naive bayes.txt', 'w', encoding='utf-8')\n",
    "for line in misclassified:\n",
    "    file.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_indexes = np.where(y_test != linear_svc_model.predict(X_test_final))\n",
    "misclassified = [('Malicious' if int(y_test[i]) == 1 else 'Legitimate') + ': ' + X_test[i] for i in misclassified_indexes[0].tolist()]\n",
    "misclassified.sort()\n",
    "misclassified\n",
    "\n",
    "file = open('Missclassified\\Linear Support Vector Classification.txt', 'w', encoding='utf-8')\n",
    "for line in misclassified:\n",
    "    file.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_indexes = np.where(y_test != logistic_model.predict(X_test_final))\n",
    "misclassified = [('Malicious' if int(y_test[i]) == 1 else 'Legitimate') + ': ' + X_test[i] for i in misclassified_indexes[0].tolist()]\n",
    "misclassified.sort()\n",
    "misclassified\n",
    "\n",
    "file = open('Missclassified\\Logistic Regression classifier.txt', 'w', encoding='utf-8')\n",
    "for line in misclassified:\n",
    "    file.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    4.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   21.6s finished\n"
     ]
    }
   ],
   "source": [
    "misclassified_indexes = np.where(y_test != random_forest_model.predict(X_test_final))\n",
    "misclassified = [('Malicious' if int(y_test[i]) == 1 else 'Legitimate') + ': ' + X_test[i] for i in misclassified_indexes[0].tolist()]\n",
    "misclassified.sort()\n",
    "misclassified\n",
    "\n",
    "file = open('Missclassified\\Random forest classifier.txt', 'w', encoding='utf-8')\n",
    "for line in misclassified:\n",
    "    file.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving models\n",
    "import pickle\n",
    "pickle.dump(mnb_model, open('Trained Models\\mnb_model', 'wb'))\n",
    "pickle.dump(linear_svc_model, open('Trained Models\\linear_svc_model', 'wb'))\n",
    "pickle.dump(logistic_model, open('Trained Models\\logistic_model', 'wb'))\n",
    "pickle.dump(random_forest_model, open('Trained Models\\\\random_forest_model', 'wb'))\n",
    "with open(\"Trained Models\\Saved_CountVectorizer.pkl\", 'wb') as handle:\n",
    "                    pickle.dump(cv, handle)\n",
    "        \n",
    "#loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
